# -*- coding: utf-8 -*-
"""Reto Limpieza del Conjunto de Datos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oQE-kt19ZSo9Wh70ziWJZgMmF0KF32PZ

# **Limpieza del Conjunto de Datos**

## Integrantes del equipo

Eduardo Rodriguez Lopez
Diego Armando Ulibarri Hernandez
Maria Fernanda Ramirez Barragan
Raul Youthan Irigoyen Osorio
Renata Montserrat De Luna Flore
Roberto Valdez Jasso

### Instrucciones:

1.  Descarguen el sets de datos del reto y analícenlo en equipo antes de empezar a limpiarlo.
2.    Limpien los datos usando herramientas de ETL.
3.    Expliquen y documenten cada decisión que hayan tomado sobre cómo limpiar los atributos y valores. Sean claros en sus explicaciones y concretos, respuestas ambiguas no serán tomadas en cuenta.
4.    Suban los scripts utilizados para limpiar los valores y su documentación a su repositorio de equipo del reto.

# **Importación de librerias**
"""

#Librerias necesarias para la actividad

import pandas as pd
import numpy as np 
import pylab
import scipy.stats as stats 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from google.colab import files
import random

"""# **Instalación API kaggle**

Nos Conectamos a la API de Kaggle
Como tambien
"""

!pip install -q kaggle

# Agregamos el API Key de Kaggle
# Nota: Cada quien debe tener su propia key diaria para poder tener
# acceso a los datos de kaggle.

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

"""# **Descarga del DataSet**"""

!kaggle competitions download -c spaceship-titanic

!unzip spaceship-titanic.zip -d "/content/kaggledbtitanic"

"""# **Analisis de datos**"""

# Para poder ver todo los datos disponibles del dataframe
# corre esta celda
pd.set_option('display.max_rows',None)
pd.set_option('display.max_columns',None)

# Generamos la el Dataframe/Dataset 
# Con el csv  kagglebtitanic csv

df_original_space_titanic = pd.read_csv("/content/kaggledbtitanic/train.csv")
#Una vez ya creado, vemos los primeros 10  datos de la tabla
df_original_space_titanic.head(10)

"""Revisamos los datos primero, para asi ver con que estamos trabajando."""

# Primero Hay que denotar el tipo de variables con las que vamos trabajar

df_original_space_titanic.info()

# Vamos  ver con que tantos datos  (filas) estamos trabajando
len(df_original_space_titanic)
# Tenemos 8693 filas con datos (no sabemos si estam bien o mal, pero existen)

df_original_space_titanic.shape # 8693 Filas y 14 columnas

# Ahora veremos  cuales tiene valores vacios en sus filas

df_original_space_titanic.isnull().sum()


# Vemos que si tiene valores vacios en gran parte de las columnas
# eso es bueno y malo al mismo tiempo, ya que no debemos preocuparmos (por ahora)
# por valores vacios, ahora bien hay que preocuparnos que el dataset tenga 
# inconsitencias en las mismas

# Veamos que tanto datos (en porcentaje) tenemos perdidos/faltantes

df_original_space_titanic.isnull().sum() * 100/df_original_space_titanic.shape[0]

#En si por columna que tenemos vacia se tiene almenos un 2% de los datos en su totalidad
# eso es bueno y malo al mismo tiempo, ya que no debemos preocuparmos (por ahora)
# por valores vacios, ahora bien hay que preocuparnos que el dataset tenga 
# inconsitencias en las mismas.

# Para Checar lo anterior debemos buscar valores unicos en el dataset
# Vamos a Checarlos todos  (Por cada Colummnas)

print(f'Columna PassengerId \n{df_original_space_titanic["PassengerId"].unique()}')
print('/----------------------------------------/')

print(f'Columna HomePlanet \n{df_original_space_titanic["HomePlanet"].unique()}')
print('/----------------------------------------/')

print(f'Columna CryoSleep \n{df_original_space_titanic["CryoSleep"].unique()}')
print('/----------------------------------------/')

print(f'Columna Cabin \n{df_original_space_titanic["Cabin"].unique()}')
print('/----------------------------------------/')

print(f'Columna Destination \n{df_original_space_titanic["Destination"].unique()}')
print('/----------------------------------------/')

print(f'Columna Age \n{df_original_space_titanic["Age"].unique()}')
print('/----------------------------------------/')

print(f'Columna VIP \n{df_original_space_titanic["VIP"].unique()}')
print('/----------------------------------------/')

print(f'Columna RoomService \n{df_original_space_titanic["RoomService"].unique()}')
print('/----------------------------------------/')

print(f'Columna FoodCourt \n{df_original_space_titanic["FoodCourt"].unique()}')
print('/----------------------------------------/')

print(f'Columna ShoppingMall \n{df_original_space_titanic["ShoppingMall"].unique()}')
print('/----------------------------------------/')

print(f'Columna Spa \n{df_original_space_titanic["Spa"].unique()}')
print('/----------------------------------------/')

print(f'Columna VRDeck \n{df_original_space_titanic["VRDeck"].unique()}')
print('/----------------------------------------/')

print(f'Columna Name \n{df_original_space_titanic["Name"].unique()}')
print('/----------------------------------------/')

print(f'Columna Transported \n{df_original_space_titanic["Transported"].unique()}')
print('/----------------------------------------/')

# Conteo de Valores
print(df_original_space_titanic.columns.values)
print('/----------------------------------------/')
print(df_original_space_titanic['HomePlanet'].value_counts())
print('/----------------------------------------/')
print(df_original_space_titanic['Destination'].value_counts())
print('/----------------------------------------/')
print(df_original_space_titanic['VIP'].value_counts())
print('/----------------------------------------/')
print(df_original_space_titanic['Transported'].value_counts())
print('/----------------------------------------/')
print(df_original_space_titanic['Cabin'].str[0].value_counts())
print('/----------------------------------------/')
print(df_original_space_titanic['CryoSleep'].value_counts())
print('/----------------------------------------/')

# Descripcion estadistica entera del dataset
df_original_space_titanic.describe()

# Checamos si hay duplicados
dupli = df_original_space_titanic.duplicated().sum()
dupli # Al parecer no hay duplicados
      # lo cual es bueno y podemos denotar que cada valor es unico
     
#elim = df_auto_original.drop_duplicates()
#elim # Si hacemos esto se mantiene igual debido a que o hay duplicados

# Correlacion de las columnas entre si.
corr = df_original_space_titanic.corr()
corr

# Graficacion de los datos correlacionados.
ax = sns.heatmap(
    corr, 
    vmin=-1, vmax=1, center=0,
    cmap=sns.diverging_palette(20, 220, n=200),
    square=True
)
ax.set_xticklabels(
    ax.get_xticklabels(),
    rotation=45,
    horizontalalignment='right'
);

"""##Distribuccion de datos



Aqui veremos como estan las distribucciones de las variables en base a los datos disponibles
"""

# Generamos una figura para graficar todas las columnas con datos (posibles) 
# numericas para visualizarlas y ver con que estamos trabajando
fig, ax = plt.subplots(figsize=(10,10))
df_original_space_titanic.hist(ax=ax)
plt.show() # presentamos la graficacion

# Generamos una figura para graficar todas las columnas con datos (posibles) 
# numericos para visualizarlas y ver con los valores atipicos disponibles
fig, ax = plt.subplots(figsize=(10,10))
df_original_space_titanic.boxplot(ax=ax)
plt.xticks(rotation=90)
plt.show()

"""# Label Encoder / OneHotEncoder

Antes de Realizar una Imputacion de los datos faltantes, es importante tambien trabajar con los datos/variables categoricas que se puede utilizar mas adelante para su analisis utiles y predicciones mas adelante.

Esto se realiza para evitar, que al momento de imputar por medio de la media, mediana, promedio  modas , entre otras formas, generar problemas que esten sean posibles por las  variables categoricas presentes.

Ahora bien, las columnas Ok para realizar el Label Enconder

1. HomePlanet 	
    
    Descripcion: El planeta del que partió el pasajero, típicamente su planeta de residencia permanente.

    Tiene Vacios: SI -  201

    Metodo: OneHotEncoderFeature
2. CryoSleep 	
    
    Descripcion: Indica si el pasajero eligió ser puesto en animación suspendida durante la duración del viaje. Los pasajeros en criosueño están confinados en sus cabinas.

    Tiene Vacios: SI - 217

    Metodo: OneHotEncoderFeature 

3. Destination	
    
    Descripcion: El planeta al que desembarcará el pasajero.

    Tiene Vacios: SI - 182

    Metodo: OneHotEncoderFeature 

4. VIP
    
    Descripcion: Si el pasajero ha pagado por un servicio VIP especial durante el viaje.

    Tiene Vacios:  SI - 203

    Metodo: OneHotEncoderFeature

5. Name	 
    
    Descripcion: Los nombres y apellidos del pasajero.

    Tiene Vacios:  SI 200 

    Tiene Importancia: no, porque no tiene relacion directa con losd datos analizar

    Metodo: LabelEncoder (Generadara MUCHOS  valores unicos)

6. Transported  
    
    Descripcion: Si el pasajero fue transportado a otra dimensión. Este es el objetivo, la columna que está tratando de predecir.

    Tiene Vacios: NO 

    Tiene Importancia:  SI, buscamos saber si los pasajeros tuvieron y tendran la posiblidad de pasar a la otra dimension.
"""

# Copia del dataframe Original 
labelEncoder_copy_Data = df_original_space_titanic.copy()

missing_feature_freq = ['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP']

# Rellenado de valores por frecuencia de  los mismos
def missing_fill(df):
    for feature in missing_feature_freq:
        most_freq = df[feature].value_counts().index[0]
        df[feature] = df[feature].fillna(most_freq)
    return df

# Extraccion de los dato de la columna  cabin
# Extraccion de la cabina inicial
def extractMainDeck(s):
    return s.split('/')[0]
# Extraccion de la del numero de cabina
def extractNumber(s):
    return s.split('/')[1]
# Extraccion de del area de la cabina (lado de la nave espacial (Ventana o pasillo))
def extractSide(s):
    return s.split('/')[2]

# Tumbar columnas no Necesarias
def drop_columns(df):
    drop_column = ['PassengerId', 'Cabin', 'Name'] # Duda con el PassengerId
    for ft in drop_column:
        df = df.drop(ft, axis = 1)
    return df

# LabelEncoder
def encoder(df):
    # Columnas para el labeLEncoder
    bool_columns = ['CryoSleep', 'VIP']
    dum_columns = ['Deck']
    
    # Por cada de las columnas los pasamos a string
    for bool_ft in bool_columns:
        df[bool_ft] = df[bool_ft].astype('str')
    # Por cada de las columnas generamos el label Encoder
    # y de ahi los  pasamos a int    
    for dum in dum_columns:
        LabelEnco = LabelEncoder()
        LabelEnco.fit_transform(df[dum])    
    df['Num'] = df['Num'].astype('int')
    # gereramos dummies
    df = pd.get_dummies(df)
    return df # y regresamos el dataframe

def categoricalPreprossesing(df):
    # Llamada de las funciones
    df = missing_fill(df)
    # Llamada de la funcion de los datos de la columna cabina
    df['Deck'] = df['Cabin'].apply(extractMainDeck)
    df['Num'] = df['Cabin'].apply(extractNumber)
    df['Side'] = df['Cabin'].apply(extractSide)
    df = drop_columns(df) # Tumbamos las columnas
    df = encoder(df) # Label Encoder
    
    return df # regresamos el df ya finalmente modificado

df_original_space_titanic.head(10)

categorical_trasformation = categoricalPreprossesing(labelEncoder_copy_Data)
categorical_trasformation.head(10)

# Checamos los vacios de ls nuevos datos numericos
categorical_trasformation.isnull().sum()
# ya quedaron todos datos categoricos a numericos, solo fataria los
# datos numericos originales y asi crear el modelo de los datos

label_encoder = LabelEncoder() # llamamos al encoder
# Columnas variable para Label Encoder

# Label encoder con la columna Transported
# Transported
transported = categorical_trasformation['Transported']
transported_encoder = label_encoder.fit_transform(transported)
transported_DF = pd.DataFrame(transported_encoder, columns=['Transported']) # dataframe
categorical_trasformation.drop(columns=['Transported'], inplace = True)
df_new_trasform_space_titanic= pd.concat([categorical_trasformation,transported_DF], axis = 1, verify_integrity=True)

# Checamos que efecto los datos se realizan de manera correcta
df_new_trasform_space_titanic.head(5)

"""# Nota: 
Realizamos el siguiente label Enconder para ver la correlacion entre el nombre y la traportacion
"""

# Generando la traduccion de categoricos a  numericos

name = df_original_space_titanic['Name']
name_encoder = label_encoder.fit_transform(name)
name = pd.DataFrame(name_encoder, columns=['Name']) # Dataframe

transported = df_original_space_titanic['Transported']
transported_encoder = label_encoder.fit_transform(transported)
transported = pd.DataFrame(transported_encoder, columns=['Transported']) # dataframe

# Borramos las columnas originales
test = df_original_space_titanic.drop(columns=['Name','Transported'], inplace = True)
#Concatenamos los nuevos datframes al original
test = pd.concat([name,test,transported], axis = 1, verify_integrity=True)

sns.jointplot(test['Name'], test['Transported'])
# Ya visto el grafico podemos denotar que que ambas columnas no tiene correlacion
# por lo tanto no lo tomamos en cuenta para la prueba y prediccion mas 
# adelante.

"""# **Imputacion de datos**


Ahora si, generamos la imputacion de los datos fatantes numericos
por medio 


de la:

1. Media
2. Mediana
3. Moda

# **Por Media**
"""

# Generamos una copia del dataframe con los datos  categoricos trasformados
imputeData_copy_Data = df_new_trasform_space_titanic.copy()
#imputeData_copy_Data.head(10)

# Realizamos la modelacion y pruebas de imputacion de datos por  la media

#Proceso de imputación de datos con la media
room_service_mean = imputeData_copy_Data['RoomService'].mean()
imputeData_copy_Data['RoomService'] = imputeData_copy_Data['RoomService'].fillna(room_service_mean)

food_court_mean = imputeData_copy_Data['FoodCourt'].mean()
imputeData_copy_Data['FoodCourt'] = imputeData_copy_Data['FoodCourt'].fillna(food_court_mean)

shopping_mall_mean = imputeData_copy_Data['ShoppingMall'].mean()
imputeData_copy_Data['ShoppingMall'] = imputeData_copy_Data['ShoppingMall'].fillna(shopping_mall_mean)

spa_mean = imputeData_copy_Data['Spa'].mean()
imputeData_copy_Data['Spa'] = imputeData_copy_Data['Spa'].fillna(spa_mean)

vrdeck_mean = imputeData_copy_Data['VRDeck'].mean()
imputeData_copy_Data['VRDeck'] = imputeData_copy_Data['VRDeck'].fillna(vrdeck_mean)

age_mean = imputeData_copy_Data['Age'].mean()
imputeData_copy_Data['Age'] = imputeData_copy_Data['Age'].fillna(age_mean)

# Ha algunos casos que hay  personas que  tiene 0 años
# arreglamos eso de la siguiente manera:
imputeData_copy_Data['Age'].replace({0.0000: age_mean}, inplace = True)

imputeData_copy_Data['Age'] = imputeData_copy_Data['Age'].astype(int)

"""# **Por Mediana**

"""

imputeData_copy_Data_median = df_new_trasform_space_titanic.copy()
imputeData_copy_Data_median.head(10)

# Proceso de imputación con la mediana
room_service_median = imputeData_copy_Data_median['RoomService'].median()
imputeData_copy_Data_median['RoomService'] = imputeData_copy_Data_median['RoomService'].fillna(room_service_median)

age_median = imputeData_copy_Data_median['Age'].median()
imputeData_copy_Data_median['Age'] = imputeData_copy_Data_median['Age'].fillna(age_median)

food_court_median = imputeData_copy_Data_median['FoodCourt'].median()
imputeData_copy_Data_median['FoodCourt'] = imputeData_copy_Data_median['FoodCourt'].fillna(food_court_median)

shopping_mall_median = imputeData_copy_Data_median['ShoppingMall'].median()
imputeData_copy_Data_median['ShoppingMall'] = imputeData_copy_Data_median['ShoppingMall'].fillna(shopping_mall_median)

spa_median = imputeData_copy_Data_median['Spa'].median()
imputeData_copy_Data_median['Spa'] = imputeData_copy_Data_median['Spa'].fillna(spa_median)

vrdeck_median = imputeData_copy_Data_median['VRDeck'].median()
imputeData_copy_Data_median['VRDeck'] = imputeData_copy_Data_median['RoomService'].fillna(vrdeck_median)

imputeData_copy_Data_median.isna().sum()

#Para personas que tienen 0 años
imputeData_copy_Data_median['Age'].replace({0.0000: age_median}, inplace = True)

imputeData_copy_Data_median['Age'] = imputeData_copy_Data_median['Age'].astype(int)

"""# **Por Moda**"""

imputeData_copy_Data_mode = df_new_trasform_space_titanic.copy()

# Proceso de imputación con la moda
room_service_mode = imputeData_copy_Data_mode['RoomService'].mode()
imputeData_copy_Data_mode['RoomService'] = imputeData_copy_Data_mode['RoomService'].fillna(room_service_mode[0])

age_mode = imputeData_copy_Data_mode['Age'].mode()
imputeData_copy_Data_mode['Age'] = imputeData_copy_Data_mode['Age'].fillna(age_mode[0])

food_court_mode = imputeData_copy_Data_mode['FoodCourt'].mode()
imputeData_copy_Data_mode['FoodCourt'] = imputeData_copy_Data_mode['FoodCourt'].fillna(food_court_mode[0])

shopping_mall_mode = imputeData_copy_Data_mode['ShoppingMall'].mode()
imputeData_copy_Data_mode['ShoppingMall'] = imputeData_copy_Data_mode['ShoppingMall'].fillna(shopping_mall_mode[0])

spa_mode = imputeData_copy_Data_mode['Spa'].mode()
imputeData_copy_Data_mode['Spa'] = imputeData_copy_Data_mode['Spa'].fillna(spa_mode[0])

vrdeck_mode = imputeData_copy_Data_mode['VRDeck'].mode()
imputeData_copy_Data_mode['VRDeck'] = imputeData_copy_Data_mode['RoomService'].fillna(vrdeck_mode[0])

# Remplazando la edad de las personas con 0 años
imputeData_copy_Data_mode['Age'].replace({0.0000: age_mode[0]}, inplace = True)

imputeData_copy_Data_mode.isna().sum()

# Convirtiendo los datos de age en enteros
imputeData_copy_Data_mode['Age'] = imputeData_copy_Data_mode['Age'].astype(int)

"""# Analisis de los resultados de la imputacion de los datos

finalmente, checamos cuál es el mejor método para imputar los datos faltantes
en base al tipo de dato presentado en cada columna, como tambien los datos 
"""

# Analisis de Room Services
print('Room service mean: ', str(room_service_mean))
print('Room service median: ', str(room_service_median))
print('Room service mode: ', str(room_service_mode[0]))
print('/-------------------------------------------/')
# Analisis del datos originales
print('Datos originales: ')
print(df_original_space_titanic['RoomService'].head(10) )
print('/-------------------------------------------/')

# Nuevo Dataset: 
imputeData_new_Data = df_new_trasform_space_titanic.copy()
imputeData_new_Data.head(5)

"""El mejor método para imputar los datos en la columna de Room service es la media, ya que la mediana y la moda tienen valor de 0.0 y podría afectar los resultados."""

# agregamos los nuevos datos de la media a los datos faltantes
imputeData_new_Data['RoomService'] = imputeData_new_Data['RoomService'].fillna(room_service_mean)
#imputeData_new_Data.head(5)

# Analisis de Age
print('Age mean: ', str(age_mean))
print('Age median: ', str(age_median))
print('Age mode: ', str(age_mode[0]))
print('/-------------------------------------------/')
# Analisis del datos originales
print('Datos originales: ')
print(df_original_space_titanic['Age'].head(10) )
print('/-------------------------------------------/')

"""En el caso de la edad, elegiremos la mediana."""

# agregamos los nuevos datos de la mediana a los datos faltantes
imputeData_new_Data['Age'] = imputeData_new_Data['Age'].fillna(age_median)
#imputeData_new_Data.head(5)

# Analisis de FoodCourt
print('Food Court mean: ', str(food_court_mean))
print('Food Court median: ', str(food_court_median))
print('Food Court mode: ', str(food_court_mode[0]))
print('/-------------------------------------------/')
# Analisis del datos originales
print('Datos originales: ')
print(df_original_space_titanic['FoodCourt'].head(10) )
print('/-------------------------------------------/')

"""Para la columna de Food Court, utilizaremos la media para la imputación de datos, ya que los ceros pueden alterar los resultados. """

imputeData_new_Data['FoodCourt'] = imputeData_new_Data['FoodCourt'].fillna(food_court_mean)

# Analisis de Shopping Mall
print('Shopping Mall mean: ', str(shopping_mall_mean))
print('Shopping Mall median: ', str(shopping_mall_median))
print('Shopping Mall mode: ', str(shopping_mall_mode[0]))
print('/-------------------------------------------/')
# Analisis del datos originales
print('Datos originales: ')
print(df_original_space_titanic['ShoppingMall'].head(10) )
print('/-------------------------------------------/')

"""Para la imputación de datos de la columna Shopping Mall, utilizaremos la media."""

imputeData_new_Data['ShoppingMall'] = imputeData_new_Data['ShoppingMall'].fillna(shopping_mall_mean)

# Analisis de Spa
print('Spa mean: ', str(spa_mean))
print('Spa median: ', str(spa_median))
print('Spa mode: ', str(spa_mode[0]))
print('/-------------------------------------------/')
# Analisis del datos originales
print('Datos originales: ')
print(df_original_space_titanic['Spa'].head(10) )
print('/-------------------------------------------/')

imputeData_new_Data['Spa'] = imputeData_new_Data['Spa'].fillna(spa_mean)

# Analisis de VRDeck
print('VRDeck mean: ', str(vrdeck_mean))
print('VRDeck median: ', str(vrdeck_median))
print('VRDeck mode: ', str(vrdeck_mode[0]))
print('/-------------------------------------------/')
# Analisis del datos originales
print('Datos originales: ')
print(df_original_space_titanic['VRDeck'].head(10) )
print('/-------------------------------------------/')

imputeData_new_Data['VRDeck'] = imputeData_new_Data['VRDeck'].fillna(vrdeck_mean)

imputeData_new_Data.head(10)

# Checamos que ya todos los datos esten completos
imputeData_new_Data.isna().sum()

# Por ultimo generamos un nuevo dataframe para su uso en el despliegue del
# modelo de inteligencia artificial

# Nuevo Dataframe: 
df_new_space_titanic_fullData = imputeData_new_Data.copy()
df_new_space_titanic_fullData.head(5)

# Checamos que ya todos los datos esten completos
imputeData_new_Data.isna().sum()